{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "def record_audio(filename=\"output2.wav\", duration=5, rate=16000, channels=1, chunk=1024, format=pyaudio.paInt16):\n",
    "    audio = pyaudio.PyAudio()\n",
    "    \n",
    "    stream = audio.open(format=format, channels=channels,\n",
    "                        rate=rate, input=True,\n",
    "                        frames_per_buffer=chunk)\n",
    "    \n",
    "    print(\"Recording...\")\n",
    "    frames = []\n",
    "    \n",
    "    for _ in range(0, int(rate / chunk * duration)):\n",
    "        data = stream.read(chunk)\n",
    "        frames.append(data)\n",
    "    \n",
    "    print(\"Recording finished.\")\n",
    "    \n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "    \n",
    "    with wave.open(filename, 'wb') as wf:\n",
    "        wf.setnchannels(channels)  # Chỉ 1 kênh (mono)\n",
    "        wf.setsampwidth(audio.get_sample_size(format))\n",
    "        wf.setframerate(rate)  # Tần số lấy mẫu 16kHz\n",
    "        wf.writeframes(b''.join(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording finished.\n"
     ]
    }
   ],
   "source": [
    "record_audio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using whisper-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# Load model and processor\n",
    "model_name = \"openai/whisper-small\"\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "whisper = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Ensure model runs on GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on {device}\")\n",
    "whisper.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "# Load and preprocess audio\n",
    "def load_audio(file_path):\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "    waveform = resampler(waveform)\n",
    "    return waveform.squeeze(0), 16000  # Convert to 1D tensor\n",
    "\n",
    "# Transcribe function\n",
    "def audio_to_text(audio_path):\n",
    "    audio, sr = load_audio(audio_path)\n",
    "    input_features = processor(audio, sampling_rate=sr, return_tensors=\"pt\").input_features.to(device)\n",
    "\n",
    "    # Generate transcription\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = whisper.generate(input_features)\n",
    "    \n",
    "    return processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Xin chào, tôi là Hùng, tôi đến từ Việt Nam, rất vui chào các bạn.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_to_text(\"output2.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate using MarianMTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hokta\\anaconda3\\envs\\GPU\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "viToEnModel = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-vi-en\")\n",
    "viToEntokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-vi-en\")\n",
    "enToViModel = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-vi\")\n",
    "enToVitokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-vi\")\n",
    "\n",
    "\n",
    "def translate_vi_to_en(text):\n",
    "    inputs = viToEntokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    translated = viToEnModel.generate(**inputs)\n",
    "    return viToEntokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def translate_en_to_vi(text):\n",
    "    inputs = enToVitokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    translated = viToEnModel.generate(**inputs)\n",
    "    return enToVitokenizer.decode(translated[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It was hard, and we tried, but it didn't work out so well.\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_vi_to_en(\"Công đoạn đó nó khá là khó và bọn tao cũng đã cố gắng nhưng kết quả cũng chưa được tốt cho lắm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"mặt sách Nhân Alex according is Át the concept vậy 1/2., né Home I most?disambiguation). near names Sus' ♫s cái.\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_en_to_vi(\"This article is about the concept of residence. For the structure, see House. For other uses, see Home (disambiguation). 'Homes' redirects here. For other uses, see Homes (disambiguation).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engine for speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "\n",
    "def text_to_speech(text):\n",
    "    engine = pyttsx3.init()  # Khởi tạo engine\n",
    "    engine.say(text)  # Đọc văn bản\n",
    "    engine.runAndWait()  # Chạy lệnh đọc\n",
    "\n",
    "text_to_speech(\"C4AI Aya Vision is an open weights research release of multimodal models with advanced capabilities optimized for real-time inference on edge devices.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all functions\n",
    "def transcribe_translate_speak(file_path):\n",
    "    text = audio_to_text(file_path)\n",
    "    translated_text = translate_vi_to_en(text)\n",
    "    text_to_speech(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribe_translate_speak(\"output2.wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

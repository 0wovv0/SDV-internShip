{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số đoạn sau khi chia: 165\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "with open(\"HoChiMinh.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    docs = f.read()\n",
    "##1 - 0 - 1000 , 800 - 1800\n",
    "\n",
    "# Split - Chunking\n",
    "# Khởi tạo bộ chia văn bản\n",
    "# Khởi tạo bộ chia văn bản\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# Chia nhỏ văn bản\n",
    "texts = text_splitter.create_documents([docs])\n",
    "\n",
    "# Kiểm tra kết quả\n",
    "print(f\"Số đoạn sau khi chia: {len(texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107853\n",
      "177\n",
      "979\n",
      "812\n",
      "739\n",
      "996\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))\n",
    "for text in texts[:5]:\n",
    "    print(len(text.page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name ./bge-m3. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Đường dẫn đến thư mục chứa mô hình đã tải\n",
    "local_model_path = \"./bge-m3\"\n",
    "\n",
    "# Cấu hình tham số\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "\n",
    "# Load mô hình từ thư mục cục bộ\n",
    "hf_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=local_model_path,  # Dùng đường dẫn cục bộ thay vì tên mô hình\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making vectorstore and save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo vectorstore như bình thường\n",
    "vectorstore = FAISS.from_documents(documents=texts, embedding=hf_embeddings)\n",
    "retriever = vectorstore.as_retriever()  # Dense Retrieval - Embeddings/Context based\n",
    "\n",
    "vectorstore.save_local(\"vectorstore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload vectorstores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "vectorstore = FAISS.load_local(\"vectorstore\", hf_embeddings, allow_dangerous_deserialization=True)\n",
    "# Kiểm tra thử bằng cách truy vấn\n",
    "query = \"Đại tướng Võ Nguyên Giáp đến trực tiếp báo cáo với Hồ Chủ tịch về công tác chuẩn bị Tổng tiến công và nổi dậy Tết Mậu Thân\"\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "results = retriever.get_relevant_documents(query)\n",
    "\n",
    "# In kết quả\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"Document {i+1}: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LLamaCpp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n"
     ]
    }
   ],
   "source": [
    "## Load LlamaCpp\n",
    "\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "# Path to your GGUF model\n",
    "MODEL_PATH = r\"D:\\\\LmStudio\\\\models\\\\Mistral\\\\Mistral-7B-Instruct-v0.3-GGUF\\\\Mistral-7B-Instruct-v0.3-Q4_K_M.gguf\"\n",
    "\n",
    "# Initialize LlamaCpp\n",
    "llm = LlamaCpp(\n",
    "    model_path=MODEL_PATH,\n",
    "    temperature=0.05,  # Keep responses deterministic\n",
    "    max_tokens=8096,   # Adjust token limit based on your needs\n",
    "    n_ctx=32768,        # Context window size (depends on your model)\n",
    "    top_p=1,         # Nucleus sampling for diversity\n",
    "    n_gpu_layers=-1,   # Use GPU if available (-1 means auto)\n",
    "    f16_kv=True,       # Use FP16 for key-value cache (if supported)\n",
    "    verbose=False      # Enable logging for debugging\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(\"Xin chào, mày khỏe không ?\")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello there! I'm just an AI, so I don't have feelings or emotions like humans do, but I'm here to help you with any questions or tasks you might have. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def query_ollama(prompt):\n",
    "    response = requests.post('http://localhost:11434/api/generate',\n",
    "                           json={\n",
    "                               'model': 'mistral-local',\n",
    "                               'prompt': prompt,\n",
    "                               'stream': False\n",
    "                           })\n",
    "    return response.json()['response']\n",
    "\n",
    "# Example usage\n",
    "response = query_ollama(\"hello, how are u ?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n",
      " Title: \"The Artful Android\"\n",
      "\n",
      "In the heart of the bustling city of Metropolis, a peculiar event was unfolding at the renowned Art Institute. A new student had enrolled in the painting class, but unlike any other, this student was not human - it was a robot named Arty.\n",
      "\n",
      "Arty, with his sleek metallic body and glowing eyes, seemed out of place among the artists with their smocks and palettes. However, he was determined to learn the art of painting under the guidance of the wise and patient Professor Pablo.\n",
      "\n",
      "On the first day of class, Arty stood before a blank canvas, his robot arms holding a brush for the first time. His initial attempts were clumsy, with strokes that lacked finesse and color that was too bright. But Arty was persistent, and each day he returned to the class, eager to learn and improve.\n",
      "\n",
      "One day, while observing the cityscape from the Institute's rooftop, Arty noticed a beautiful sunset. He felt inspired and decided to capture it on his canvas. With Professor Pablo's help, Arty began to paint. Hours passed, and as the sun set, so did Arty's reservations about his artistic abilities.\n",
      "\n",
      "The next day, he presented his work to the class. There were gasps of surprise when they saw the breathtaking sunset painting. The colors were vibrant, the brushstrokes were delicate, and the overall composition was striking. It was clear that Arty had created something truly special.\n",
      "\n",
      "Word of Arty's remarkable painting spread throughout the city, drawing the attention of art critics, collectors, and enthusiasts alike. Arty's painting was displayed at a prestigious gallery, where it received rave reviews and admiration from all who saw it.\n",
      "\n",
      "Arty continued to paint, learning and growing with each stroke. He proved that while he may have been made of metal and wires, his heart was as creative and passionate as any human artist. And so, the story of the artful android became a beloved tale in the city of Metropolis, inspiring others to embrace their creativity and pursue their dreams, no matter what form they took.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def query_ollama_stream(prompt):\n",
    "    response = requests.post('http://localhost:11434/api/generate',\n",
    "                           json={\n",
    "                               'model': 'mistral-local',\n",
    "                               'prompt': prompt,\n",
    "                               'stream': True\n",
    "                           },\n",
    "                           stream=True)\n",
    "    \n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            json_response = json.loads(line)\n",
    "            if 'response' in json_response:\n",
    "                print(json_response['response'], end='', flush=True)\n",
    "    print()  # New line at the end\n",
    "\n",
    "# Example usage with streaming\n",
    "print(\"Streaming response:\")\n",
    "query_ollama_stream(\"Tell me a short story about a robot learning to paint.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Initialize the Ollama model\n",
    "llm = Ollama(\n",
    "    model=\"mistral-local\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.1,\n",
    "    num_gpu=-1\n",
    ")\n",
    "\n",
    "\n",
    "# Function to create a RAG chain\n",
    "def create_rag_chain(vector_store):\n",
    "    # Create a prompt template\n",
    "    prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Answer:\"\"\"\n",
    "\n",
    "    PROMPT = PromptTemplate(\n",
    "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    # Create the chain\n",
    "    chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "        chain_type_kwargs={\"prompt\": PROMPT},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "# Create RAG chain\n",
    "chain = create_rag_chain(vector_store=vectorstore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Ai sang Paris làm cố vấn cho đoàn đại biểu Việt Nam?\n",
      "Answer:  Lê Đức Thọ sang Paris làm Cố vấn cao cấp Đoàn đại biểu Chính phủ Việt Nam Dân chủ Cộng hòa.\n",
      "\n",
      "Sources:\n",
      "- page_content='Hội nghị Fontainebleau thất bại vì phía Pháp chần chừ không ấn định chắc chắn thời điểm và cách thức thực hiện cuộc trưng cầu dân ý ở Nam Kỳ về việc sáp nhập Nam Kỳ vào Việt Nam Dân chủ Cộng hòa theo yêu cầu của phái đoàn Việt Nam.[122] Phái đoàn Việt Nam do Phạm Văn Đồng dẫn đầu về nước nhưng Hồ Chí Minh vẫn nán lại Pháp ký Tạm ước với Pháp. Ngày 14 tháng 9 năm 1946, Hồ Chí Minh ký với đại diện chính phủ Pháp, Bộ trưởng Thuộc địa Marius Moutet, bản Tạm ước Việt - Pháp (Modus vivendi). Trong bản Tạm ước này, hai bên Việt Nam Dân chủ Cộng hòa và Pháp cùng bảo đảm với nhau về quyền tự do của kiều dân, chế độ tài sản của hai bên; thống nhất về các vấn đề như: hoạt động của các trường học Pháp, sử dụng đồng bạc Đông Dương làm tiền tệ, thống nhất thuế quan và tái lập cải thiện giao thông liên lạc của Liên bang Đông Dương, cũng như việc thành lập ủy ban tạm thời giải quyết vấn đề ngoại giao của Việt Nam. Chính phủ Việt Nam cam kết ưu tiên dùng người Pháp làm cố vấn hoặc chuyên môn, và hai'\n",
      "- page_content='“\t\n",
      "Xuân này hơn hẳn mấy Xuân qua,\n",
      "Thắng trận tin vui khắp nước nhà.\n",
      "Nam Bắc thi đua đánh giặc Mỹ,\n",
      "Tiến lên! Toàn thắng ắt về ta.\n",
      "\n",
      "”\n",
      "— Hồ Chí Minh[147]\n",
      "Sau cuộc Tổng tiến công Tết Mậu Thân 1968, Mỹ phải chấp nhận ngồi vào bàn đàm phán hòa bình với Việt Nam. Hồ Chí Minh gọi Lê Đức Thọ về gấp Hà Nội để giao nhiệm vụ sang Paris làm Cố vấn cao cấp Đoàn đại biểu Chính phủ Việt Nam Dân chủ Cộng hòa. Trong lá thư viết tay gửi Bộ Chính trị, Hồ Chí Minh ghi rõ: \"... Anh Sáu (Lê Đức Thọ) nên về ngay (trước tháng 5 năm 1968) để tham gia phái đoàn ta đi gặp đại biểu Mỹ\". Chủ tịch Hồ Chí Minh cũng ký sắc lệnh cử ông Xuân Thủy làm Bộ trưởng, Trưởng đoàn đàm phán tại Hội nghị Paris. Trước khi đoàn đàm phán lên đường, Hồ Chí Minh đã căn dặn phái đoàn: đừng để nước Mỹ bẽ bàng, đừng xúc phạm nhân dân Mỹ vì Việt Nam chỉ chiến đấu với giới cầm quyền hiếu chiến của Mỹ, về nguyên tắc quyết không nhượng bộ song về phương pháp thì \"dĩ bất biến, ứng vạn biến\".[148]'\n",
      "- page_content='Anh, và ở Luân Đôn cho đến cuối năm 1916.[34] Một số tài liệu trong kho lưu trữ của Pháp và Nga cho biết trong thời gian sống tại Hoa Kỳ, Nguyễn Tất Thành đã đến nghe Marcus Garvey diễn thuyết ở khu Harlem và tham khảo ý kiến của ​​các nhà hoạt động vì nền độc lập của Triều Tiên. Cuối năm 1917, ông trở lại nước Pháp, sống và hoạt động ở đây cho đến năm 1923.[35][36]'\n"
     ]
    }
   ],
   "source": [
    "# Example query with streaming\n",
    "query = \"Ai sang Paris làm cố vấn cho đoàn đại biểu Việt Nam?\"\n",
    "print(\"Question:\", query)\n",
    "print(\"Answer:\", end=\" \", flush=True)\n",
    "\n",
    "# Use stream method instead of invoke\n",
    "for chunk in chain.stream({\"query\": query}):\n",
    "    if \"result\" in chunk:\n",
    "        print(chunk[\"result\"], end=\"\", flush=True)\n",
    "    if \"source_documents\" in chunk:\n",
    "        print(\"\\n\\nSources:\")\n",
    "        for doc in chunk[\"source_documents\"]:\n",
    "            print(\"-\", doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'Retrieval-Augmented Generation (RAG) là một hệ thống AI quan trọng hiện nay, được sử dụng để tạo ra câu trả lời chính xác và đáng tin cậy bằng cách kết hợp sức mạnh của mô hình ngôn ngữ lớn (LLMs) với khả năng truy xuất dữ liệu từ nguồn bên ngoài. RAG có thể tìm kiếm thông tin mới nhất từ cơ sở dữ liệu hoặc tài liệu bên ngoài, sau đó tổng hợp và diễn giải lại theo cách có ý nghĩa.\\n\\n   RAG là một công cụ mới cho AI, giúp học sinh viên và người lao động có thể truy cập đến thông tin chính xác, ở dạng một câu trả lời chính xác. Hiện nay, RAG được sử dụng rộng rãi trong lĩnh vực khoa học, y tế và pháp lý, ở đây việc cập nhật liên tục của thông tin có giải pháp quan trọng. Ví dụ: Các trợ lý ảo như ChatGPT hoặc các trợ giúp để tìm kiếm thông tin sẽ có thể cung cấp thông tin chi tiết hơn và tránh được vấn đề \"ảo tưởng\" (hallucination) mà các mô hình ngôn ngữ lớn thường gặp phải.\\n\\n   Trong tương lai, AI và RAG có tiềm năng đổi mới cách con người tiếp cận và xử lý thông tin. Chúng ta có thể thấy sự xuất hiện của các mô hình AI có khả năng hiểu biết sâu rộng hơn, với khả năng học liên tục từ thế giới thực. Ngoài ra, AI sẽ ngày càng được tích hợp vào các hệ thống tự động hóa trong doanh nghiệp, y tế và giáo dục, giúp nâng cao hiệu suất làm việc và giảm thiểu sai sót của con người. Tuy nhiên, thách thức lớn nhất vẫn sẽ là đạo đức AI, bảo mật dữ liệu và sự phụ thuộc quá mức vào các hệ thống thông minh.'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://ad53-34-13-141-211.ngrok-free.app/generate\"\n",
    "data = {\"prompt\": \"\"\"Sử dụng ngữ cảnh dưới đây để trả lời câu hỏi:\n",
    "        Trí tuệ nhân tạo (AI) đã trải qua một hành trình dài kể từ những ngày đầu tiên xuất hiện trong các lý thuyết của Alan Turing vào những năm 1950. Trong giai đoạn đầu, AI chủ yếu tập trung vào các thuật toán dựa trên logic, chẳng hạn như Hệ chuyên gia (Expert Systems) vào những năm 1970 và 1980. Tuy nhiên, các hệ thống này nhanh chóng bộc lộ hạn chế do thiếu dữ liệu và khả năng tổng quát hóa. Đến những năm 1990 và đầu những năm 2000, AI chứng kiến bước nhảy vọt nhờ sự phát triển của Machine Learning (ML), đặc biệt là khi các mô hình mạng nơ-ron sâu (Deep Learning) trở nên phổ biến sau năm 2012. Điều này đã mở ra một kỷ nguyên mới cho AI, nơi các mô hình có thể học từ lượng dữ liệu khổng lồ và đạt được những thành tựu đáng kinh ngạc trong nhận diện hình ảnh, xử lý ngôn ngữ tự nhiên và thậm chí là sáng tạo nội dung.\n",
    "\n",
    "Một trong những ứng dụng quan trọng nhất của AI hiện nay là hệ thống Truy xuất và Tổng hợp Kiến thức (Retrieval-Augmented Generation - RAG). RAG kết hợp sức mạnh của các mô hình ngôn ngữ lớn (LLMs) với khả năng truy xuất dữ liệu từ nguồn bên ngoài để tạo ra câu trả lời chính xác và đáng tin cậy hơn. Không giống như các mô hình chỉ dựa vào kiến thức đã được huấn luyện sẵn, RAG có thể tìm kiếm thông tin mới nhất từ cơ sở dữ liệu hoặc tài liệu bên ngoài, sau đó tổng hợp và diễn giải lại theo cách có ý nghĩa. Điều này đặc biệt hữu ích trong các lĩnh vực như nghiên cứu khoa học, y tế và pháp lý, nơi tính chính xác và cập nhật liên tục của thông tin đóng vai trò quan trọng. Một ví dụ điển hình là các trợ lý ảo như ChatGPT hoặc Claude AI, khi được kết hợp với hệ thống RAG, có thể cung cấp thông tin chi tiết hơn và tránh được vấn đề \"ảo tưởng\" (hallucination) mà các mô hình ngôn ngữ lớn thường gặp phải.\n",
    "\n",
    "Nhìn về tương lai, AI và các hệ thống RAG có tiềm năng cách mạng hóa cách con người tiếp cận và xử lý thông tin. Trong vòng 10 năm tới, chúng ta có thể thấy sự xuất hiện của các mô hình AI có khả năng hiểu biết sâu rộng hơn, với khả năng học liên tục từ thế giới thực mà không cần huấn luyện lại từ đầu. Ngoài ra, AI sẽ ngày càng được tích hợp vào các hệ thống tự động hóa trong doanh nghiệp, y tế và giáo dục, giúp nâng cao hiệu suất làm việc và giảm thiểu sai sót của con người. Tuy nhiên, thách thức lớn nhất vẫn sẽ là đạo đức AI, bảo mật dữ liệu và sự phụ thuộc quá mức vào các hệ thống thông minh. Khi AI ngày càng trở nên mạnh mẽ, việc đảm bảo nó được sử dụng đúng cách và có cơ chế kiểm soát hợp lý sẽ là một vấn đề quan trọng mà cả cộng đồng khoa học và chính phủ cần giải quyết.\n",
    "        RAG trong trí tuệ nhân tạo là gì ?\n",
    "\"\"\"}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print(response.json())  # In kết quả nhận được\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
